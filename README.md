# Teste GitOps + HPA + ArgoCD (replicas, AZ, Spot)

Este teste valida o comportamento do Kubernetes + ArgoCD quando usamos **HPA** para controlar `spec.replicas`, alÃ©m de confirmar:
- valor padrÃ£o de `replicas` quando nÃ£o definido no Deployment
- conflito ArgoCD Ã— HPA (e como resolver)
- espalhamento de pods por **AZ**
- preferÃªncia por **nodes Spot**
- convivÃªncia com **amd64 + arm64**

---

## ğŸ¯ Objetivos do teste

1. Confirmar que:
   - sem `replicas` no Deployment â†’ Kubernetes usa **1 por padrÃ£o**
   - com HPA â†’ **HPA passa a controlar `spec.replicas`**
2. Validar comportamento do ArgoCD:
   - sem `ignoreDifferences` â†’ pode gerar **OutOfSync**
   - com `ignoreDifferences + RespectIgnoreDifferences` â†’ fica **Synced**
3. Verificar:
   - pods distribuÃ­dos em **AZs diferentes**
   - uso de **Spot instances**
   - suporte a **amd64 e arm64**

---

## ğŸ“¦ Setup utilizado

- **Controller GitOps**: ArgoCD
- **Autoscaling**: HPA (CPU)
- **App de teste**: nginx
- **Cluster**: EKS (sa-east-1)
- **Provisionamento de nodes**: Karpenter
- **Namespace**: `nginx-hpa-test`

---

## ğŸ§ª Manifestos usados

### Deployment
- **Sem `spec.replicas`**
- Health checks via HTTP (`/`)
- `topologySpreadConstraints` por AZ
- `nodeAffinity` preferindo Spot
- `podAntiAffinity` por hostname

### HPA
```yaml
minReplicas: 3
maxReplicas: 6
metrics:
  - cpu averageUtilization: 30
```


## ğŸ” Verificar AZ + Spot/On-Demand + Arquitetura (pod â†’ node)

Este comando mostra, para cada pod do teste:
- **NODE** onde ele estÃ¡ rodando
- **ZONE (AZ)** do node (`topology.kubernetes.io/zone`)
- **CAPACITY TYPE** do Karpenter (`karpenter.sh/capacity-type`) â†’ `spot` ou `on-demand`
- **ARCH** do node (`kubernetes.io/arch`) â†’ `amd64` ou `arm64`

### âœ… Comando

```bash
kubectl -n conexa get pods -l app=nginx-hpa-test \
  -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.nodeName}{"\n"}{end}' \
| while read -r POD NODE; do
  ZONE=$(kubectl get node "$NODE" -o jsonpath='{.metadata.labels.topology\.kubernetes\.io/zone}')
  CAP=$(kubectl get node "$NODE" -o jsonpath='{.metadata.labels.karpenter\.sh/capacity-type}')
  ARCH=$(kubectl get node "$NODE" -o jsonpath='{.metadata.labels.kubernetes\.io/arch}')
  echo -e "$POD\t$NODE\t$ZONE\t$CAP\t$ARCH"
done | column -t
````

### âœ… Exemplo de saÃ­da (resultado real do teste)

```text
nginx-hpa-test-54bcdb89f5-4scm4  ip-10-20-20-228.sa-east-1.compute.internal  sa-east-1b  spot  arm64
nginx-hpa-test-54bcdb89f5-kcwjp  ip-10-20-17-149.sa-east-1.compute.internal  sa-east-1a  spot  amd64
nginx-hpa-test-54bcdb89f5-v766s  ip-10-20-27-242.sa-east-1.compute.internal  sa-east-1c  spot  amd64
nginx-hpa-test-54bcdb89f5-wkxj7  ip-10-20-22-128.sa-east-1.compute.internal  sa-east-1b  spot  arm64
```

### âœ… InterpretaÃ§Ã£o rÃ¡pida

* **AZ**: estÃ¡ espalhado entre `sa-east-1a`, `sa-east-1b`, `sa-east-1c`
* **Spot**: todos os nodes estÃ£o como `spot` (fallback para on-demand nÃ£o foi necessÃ¡rio nesse teste)
* **Arquitetura**: mistura de `amd64` e `arm64` (testando multi-arch)
* `sa-east-1b` aparece duas vezes porque com 4 pods e 3 AZs, a distribuiÃ§Ã£o mais comum Ã© **2 / 1 / 1** (respeitando `maxSkew: 1`)



Perfeito ğŸ‘
Segue a explicaÃ§Ã£o **no formato `.md`**, pronta pra vocÃª colar no repositÃ³rio (ex: `SCHEDULING.md` ou no prÃ³prio `README.md`).


# Entendendo Affinity, Anti-Affinity e Spread por AZ (Kubernetes)

Este documento explica as regras de **agendamento de pods** usadas no teste `nginx-hpa-test`
para alcanÃ§ar **alta disponibilidade**, **uso de Spot**, e **distribuiÃ§Ã£o por AZ**.

---

## ğŸ“Œ Bloco analisado

```yaml
affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
            - key: karpenter.sh/capacity-type
              operator: In
              values: ["spot"]

    # requiredDuringSchedulingIgnoredDuringExecution:
    #   nodeSelectorTerms:
    #   - matchExpressions:
    #     - key: kubernetes.io/arch
    #       operator: In
    #       values: ["amd64"]

  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app: nginx-hpa-test
          topologyKey: kubernetes.io/hostname

topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        app: nginx-hpa-test
````

---

## 1ï¸âƒ£ Node Affinity â€” escolher **QUE TIPO DE NODE** usar

### Preferir Spot (soft rule)

```yaml
preferredDuringSchedulingIgnoredDuringExecution
```

ğŸ“– Significado:

* O scheduler **tenta** rodar o pod em nodes Spot
* Se **nÃ£o houver Spot disponÃ­vel**, ele **pode usar On-Demand**
* Nunca deixa o pod em `Pending` por falta de Spot

âœ… Comportamento desejado:

> **Spot se tiver, senÃ£o On-Demand**

---

### ForÃ§ar arquitetura (hard rule â€“ opcional)

```yaml
requiredDuringSchedulingIgnoredDuringExecution
```

ğŸ“– Significado:

* SÃ³ agenda em nodes com a arquitetura especificada (`amd64` ou `arm64`)
* Se nÃ£o existir node compatÃ­vel â†’ pod fica **Pending**

ğŸ§ª Usado apenas para testes de arquitetura.

---

## 2ï¸âƒ£ Pod Anti-Affinity â€” evitar pods no mesmo node

```yaml
podAntiAffinity:
  topologyKey: kubernetes.io/hostname
```

ğŸ“– Significado:

* Evita colocar **dois pods iguais no mesmo node**
* O domÃ­nio aqui Ã© o **hostname (node)**

ğŸ”¹ `preferred` (soft):

* Tenta espalhar
* Se nÃ£o tiver node suficiente, ainda agenda

âœ… Resultado observado:

* Um pod por mÃ¡quina sempre que possÃ­vel

---

## 3ï¸âƒ£ Topology Spread Constraints â€” espalhar por AZ

```yaml
topologyKey: topology.kubernetes.io/zone
```

ğŸ“– Significado:

* Espalha pods entre **Availability Zones**
* Ajuda a evitar perda total da aplicaÃ§Ã£o caso uma AZ caia

### `maxSkew: 1`

* DiferenÃ§a mÃ¡xima de pods entre AZs Ã© **1**
* Exemplo vÃ¡lido com 4 pods / 3 AZs:

  * 2 / 1 / 1

### `whenUnsatisfiable: ScheduleAnyway`

* Se nÃ£o der pra balancear perfeitamente:

  * **agenda mesmo assim**
  * evita pod `Pending`

---

## ğŸ¤ Como essas regras trabalham juntas

Quando o HPA escala os pods, o scheduler tenta, **nesta ordem**:

1. Preferir **Spot** (nodeAffinity)
2. Evitar 2 pods no **mesmo node** (podAntiAffinity)
3. Espalhar por **AZ** (topologySpreadConstraints)

---

## âœ… Resultado prÃ¡tico observado

* Pods distribuÃ­dos entre `sa-east-1a`, `sa-east-1b`, `sa-east-1c`
* Todos rodando em **Spot**
* Mistura de `amd64` e `arm64`
* Um pod por node quando possÃ­vel

---

## âš–ï¸ Soft vs Hard (Resumo)

| Tipo | Keyword     | Comportamento                     |
| ---- | ----------- | --------------------------------- |
| Soft | `preferred` | Tenta respeitar, mas nÃ£o bloqueia |
| Hard | `required`  | ObrigatÃ³rio, pode deixar Pending  |

---

## ğŸ§  Boas prÃ¡ticas recomendadas

* âœ… `preferred` para Spot (fallback automÃ¡tico)
* âœ… `podAntiAffinity` para HA sem risco de Pending
* âœ… `topologySpreadConstraints` para HA entre AZs
* âŒ NÃ£o usar regras `required` sem necessidade real

---

## ğŸ”— ReferÃªncias oficiais

* Kubernetes â€“ Node Affinity
  [https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)

* Kubernetes â€“ Pod Anti-Affinity
  [https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)

* Kubernetes â€“ Topology Spread Constraints
  [https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/)

```

---

Se quiser, no prÃ³ximo passo eu posso:
- ğŸ”– criar um **diagrama visual** desse agendamento
- ğŸ§© gerar uma **policy padrÃ£o** pro time (copiar/colar)
- ğŸ“¦ separar isso em **README + SCHEDULING.md + HPA.md**

Esse material jÃ¡ estÃ¡ em nÃ­vel **documentaÃ§Ã£o de time senior** ğŸ‘Œ
```
